{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법과 선형회귀\n",
    "1. 미니배치\n",
    "1. 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 모듈 임포트\n",
    "\n",
    "이전에 정의한 함수를 사용하기 위한 준비가 필요하며,\n",
    "이전에 사용한 모든 파이썬 코드가 `../scratch/` 디렉토리에 저장되어 있다고 가정한다.\n",
    "\n",
    "여기서는 선형대수 모듈에 포함되어 있는 `Vector` 자료형과 `dot` (벡터곱)함수를 불러온다.\n",
    "\n",
    "* `Vector = List[float]`\n",
    "* `dot(v:Vector, w:Vector) -> float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 선형대수 모듈로부터 Vector 자료형과 dot 함수 불러오기\n",
    "from scratch.linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 1: 경사하강법 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$의 최댓값(최솟값)을 구하고자 한다.\n",
    "예를 들어, \n",
    "실수 벡터를 인자로 받아 항목들의 제곱의 합을 계산하는 함수 `sum_of_squares()`가 아래와 같이 정의되었다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최댓값(최솟값)이 존재한다는 것이 알려졌다 하더라도 실제로 최댓값(최솟값)\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트를 방향(반대 방향)으로 조금씩 이동하면서 최댓값(최솟값) 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$가 vector $\\textbf{x}\\in \\textbf{R}$에서 \n",
    "미분 가능할 때 그레이디언트는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial x_n} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $(\\frac{\\partial}{\\partial x_1} f(\\textbf{x}), \\frac{\\partial}{\\partial x_2} f(\\textbf{x}))$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../images/tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사 하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법은 다음 과정을 반복하여 최댓값(최솟값) 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 2: 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_h \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 `square()`의 도함수 $f'(x) = 2x$는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 많지 않다.\n",
    "따라서 여기서는 많은 경우 충분히 작은 $h$에 대한 함수의 변화율을 측정하면\n",
    "미분값의 근사치를 사용할 수 있다는 사실을 확인하고자 한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `derivative()` 를 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 그래프로 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "# 실제 도함수 그래프(빨간색 점)\n",
    "plt.plot(xs, actuals, 'r.', label='Actual') \n",
    "# 근사치 그래프(검은색 +)\n",
    "plt.plot(xs, estimates, 'k+', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{x}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}f(\\mathbf x) = \\lim_h \\frac{f(\\mathbf{x}_h) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{x}_h$는 $\\mathbf x$의 $i$번째 항목에 $h$를 더한 벡터를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial x_i} f(\\mathbf x)$ 는 $x_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial x_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다.\n",
    "\n",
    "아래 코드에서 정의된 `partial_difference_quotient`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 `i`번째 편도함수의 근사치를 지정하기 위해 `i`번째 매개변수에만 `h`가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                v: Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 `h`의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "그레이디언트를 계산할 때마다 `f`를 두 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 3: 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scratch.linear_algebra import distance, add, scalar_multiply\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
    "\n",
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept    # The prediction of the model.\n",
    "    error = (predicted - y)              # error is (predicted - actual)\n",
    "    squared_error = error ** 2           # We'll minimize squared error\n",
    "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
    "    return grad\n",
    "\n",
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]\n",
    "\n",
    "# \"Using the Gradient\" example\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "    v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
    "    print(epoch, v)\n",
    "\n",
    "assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n",
    "\n",
    "\n",
    "# First \"Using Gradient Descent to Fit Models\" example\n",
    "\n",
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# Start with random values for slope and intercept.\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # Compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # Take a step in that direction\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "\n",
    "\n",
    "# Minibatch gradient descent example\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "\n",
    "\n",
    "# Stochastic gradient descent example\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
