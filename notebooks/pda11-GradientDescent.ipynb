{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법 모델\n",
    "1. 미니배치\n",
    "1. 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 모듈 임포트\n",
    "\n",
    "이전에 정의한 함수를 사용하기 위한 준비가 필요하며,\n",
    "이전에 사용한 모든 파이썬 코드가 `../scratch/` 디렉토리에 저장되어 있다고 가정한다.\n",
    "\n",
    "여기서는 선형대수 모듈에 포함되어 있는 `Vector` 자료형과 `dot` (벡터곱)함수를 불러온다.\n",
    "\n",
    "* `Vector = List[float]`\n",
    "* `dot(v:Vector, w:Vector) -> float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 선형대수 모듈로부터 Vector 자료형과 dot 함수 불러오기\n",
    "from scratch.linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 1: 경사하강법 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실수 벡터를 인자로 받아 실수를 내주는 아래 함수 `sum_of_squares`가 아래와 같이 정의되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares()` 함수가 최댓값 또는 최솟값을 갖도록 하는\n",
    "벡터 `v`를 찾아야 한다.\n",
    "\n",
    "수학적으로 최댓값이 존재한다는 것이 알려졌다 하더라도 실제로 최솟값 지점을 확인하는 일은\n",
    "경우에 따라 매우 어렵거나 불가능하다. \n",
    "이럴 때 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "다음 과정을 반복하여 최솟값 지점을 찾아간다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향으로 그레이디언트 크기의 일정 비율만큼 이동한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def square(x: float) -> float:\n",
    "    return x * x\n",
    "\n",
    "def derivative(x: float) -> float:\n",
    "    return 2 * x\n",
    "\n",
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]\n",
    "\n",
    "import random\n",
    "from scratch.linear_algebra import distance, add, scalar_multiply\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
    "\n",
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept    # The prediction of the model.\n",
    "    error = (predicted - y)              # error is (predicted - actual)\n",
    "    squared_error = error ** 2           # We'll minimize squared error\n",
    "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
    "    return grad\n",
    "\n",
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]\n",
    "\n",
    "def main():\n",
    "    xs = range(-10, 11)\n",
    "    actuals = [derivative(x) for x in xs]\n",
    "    estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "    \n",
    "    # plot to show they're basically the same\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "    plt.plot(xs, actuals, 'rx', label='Actual')       # red  x\n",
    "    plt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\n",
    "    plt.legend(loc=9)\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                    v: Vector,\n",
    "                                    i: int,\n",
    "                                    h: float) -> float:\n",
    "        \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "        w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
    "             for j, v_j in enumerate(v)]\n",
    "    \n",
    "        return (f(w) - f(v)) / h\n",
    "    \n",
    "    \n",
    "    # \"Using the Gradient\" example\n",
    "    \n",
    "    # pick a random starting point\n",
    "    v = [random.uniform(-10, 10) for i in range(3)]\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "        v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
    "        print(epoch, v)\n",
    "    \n",
    "    assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n",
    "    \n",
    "    \n",
    "    # First \"Using Gradient Descent to Fit Models\" example\n",
    "    \n",
    "    from scratch.linear_algebra import vector_mean\n",
    "    \n",
    "    # Start with random values for slope and intercept.\n",
    "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    for epoch in range(5000):\n",
    "        # Compute the mean of the gradients\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "        # Take a step in that direction\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)\n",
    "    \n",
    "    slope, intercept = theta\n",
    "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "    \n",
    "    \n",
    "    # Minibatch gradient descent example\n",
    "    \n",
    "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        for batch in minibatches(inputs, batch_size=20):\n",
    "            grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "            theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)\n",
    "    \n",
    "    slope, intercept = theta\n",
    "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "    \n",
    "    \n",
    "    # Stochastic gradient descent example\n",
    "    \n",
    "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        for x, y in inputs:\n",
    "            grad = linear_gradient(x, y, theta)\n",
    "            theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)\n",
    "    \n",
    "    slope, intercept = theta\n",
    "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "    \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
