{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법과 선형회귀\n",
    "1. 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 모듈 불러오기\n",
    "\n",
    "이전에 정의한 함수를 사용하기 위한 준비가 필요하며,\n",
    "이전에 사용한 모든 파이썬 코드가 `../scratch/` 디렉토리에 저장되어 있다고 가정한다.\n",
    "\n",
    "여기서는 선형대수 모듈에 포함되어 있는 `Vector` 자료형과 `dot` (벡터곱)함수를 불러온다.\n",
    "\n",
    "* `Vector = List[float]`\n",
    "* `dot(v:Vector, w:Vector) -> float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 선형대수 모듈로부터 Vector 자료형과 dot 함수 불러오기\n",
    "from scratch.linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 1: 경사하강법 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$의 최댓값(최솟값)을 구하고자 한다.\n",
    "예를 들어, \n",
    "길이가 $n$인 실수 벡터를 입력받아 항목들의 제곱의 합을 계산하는 함수가 다음과 같다고 하자.\n",
    "\n",
    "$$\n",
    "f(\\mathbf x) = f(x_1, ..., x_n) = \\sum_{k=1}^{n} x_k^2 = x_1^2 + \\cdots x_n^2\n",
    "$$\n",
    "\n",
    "아래 코드에서 정의된 `sum_of_squares()`가 위 함수를 파이썬으로 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최댓값(최솟값)이 존재한다는 것이 알려졌다 하더라도 실제로 최댓값(최솟값)\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트를 방향(반대 방향)으로 조금씩 이동하면서 최댓값(최솟값) 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$가 vector $\\textbf{x}\\in \\textbf{R}$에서 \n",
    "미분 가능할 때 그레이디언트는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial x_n} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $(\\frac{\\partial}{\\partial x_1} f(\\textbf{x}), \\frac{\\partial}{\\partial x_2} f(\\textbf{x}))$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../images/tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 다음 과정을 반복하여 최댓값(최솟값) 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. \n",
    "\n",
    "최솟값 지점에 가까워질 수록 그레이디언트는 점점 0벡터에 가까워진다. \n",
    "따라서 그레이디언트가 충분히 작아지면 최솟값 지점에 매우 가깞다고 판단하여 그 위치에서\n",
    "최솟값의 근사치를 구하여 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 2: 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_h \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 `square()`의 도함수 $f'(x) = 2x$는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 많지 않다.\n",
    "따라서 여기서는 많은 경우 충분히 작은 $h$에 대한 함수의 변화율을 측정하면\n",
    "미분값의 근사치를 사용할 수 있다는 사실을 확인하고자 한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `derivative()` 를 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 그래프로 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "# 실제 도함수 그래프(빨간색 점)\n",
    "plt.plot(xs, actuals, 'r.', label='Actual') \n",
    "# 근사치 그래프(검은색 +)\n",
    "plt.plot(xs, estimates, 'k+', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{x}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}f(\\mathbf x) = \\lim_h \\frac{f(\\mathbf{x}_h) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{x}_h$는 $\\mathbf x$의 $i$번째 항목에 $h$를 더한 벡터를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial x_i} f(\\mathbf x)$ 는 $x_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial x_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다.\n",
    "\n",
    "아래 코드에서 정의된 `partial_difference_quotient`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 `i`번째 편도함수의 근사치를 지정하기 위해 `i`번째 매개변수에만 `h`가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                v: Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 `h`의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "벡터 `v`의 길이가 $n$이면 `estimate_gradient()` 함수를 호출할 때마다\n",
    "`f`를 $2n$ 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 3: 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 제곱함수 `sum_of_squares()` 는 `v`가 0 벡터일 때 가장 작은 값을 갖는다. \n",
    "이 사실을 경사하강법을 이용하여 확인해보자. \n",
    "\n",
    "먼저, `sum_of_squares()` 함수의 그레이디언트는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\frac{\\partial}{\\partial x_2} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래 코드는 리스트를 이용하여 그레이디언트를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 지점(`v`)에서 계산된 그레이디언트에 스텝(step)이라는 특정 상수를 곱한 값을\n",
    "더해 새로운 지점을 계산하는 함수를 구현한다.\n",
    "\n",
    "* `add`, `scalar_multiply`, `distance` 등은 선형대수 부분에서 정의한 `linear_algebra` 모듈에서 가져온다.\n",
    "* `add(v, step)`: 스텝사이즈가 음수이면 그레이디언트가 가리키는 방향의 반대방향으로 지정된 비율만큼 움직인 벡터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scratch.linear_algebra import distance, add, scalar_multiply\n",
    "\n",
    "# v에서의 그레이디언트를 구한 후 스텝이 지정한 크기 비율과 방향으로 이동한 새로운 벡터 v'을 계산한다.\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 지점에서 출발하여 `gradient_step`을 충분히 반복하면\n",
    "제곱함수의 최솟점에 충분히 가깝게 근사할 수 있음을 아래 코드가 보여준다.\n",
    "실제로 1.0e-07보다 작다.\n",
    "\n",
    "* `random.seed(42)`: 실행할 때마다 동일한 결과를 보장해준다. 사용하지 않으면 매번 다른 결과가 나옴.\n",
    "* `grad`: 이동할 때마다 계산된 그레이디언트. 최종적으로 0 벡터에 가까운 값을 갖게 됨.\n",
    "* `if epoch%100 == 0`: 위치 이동을 100번 할 때마다 현재 위치 확인\n",
    "* `epoch`(에포크): 여기서는 그레이디언트 계산 횟수. 즉, 이동횟수를 가리킴.\n",
    "* `step_size=-0.01`: 그레이디언트 반대 방향, 즉, 최솟값 지점을 향해 이동할 때 사용되는 크기 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.732765249774521, -9.309789197635727, -4.409425359965263]\n",
      "100 [0.3624181137897112, -1.2346601088642208, -0.5847760329896551]\n",
      "200 [0.048063729299005625, -0.16374007531854073, -0.07755273779298358]\n",
      "300 [0.006374190434279768, -0.02171513607091833, -0.010285009644527733]\n",
      "400 [0.0008453423045827667, -0.002879851701919325, -0.0013639934114305214]\n",
      "500 [0.00011210892101281368, -0.00038192465375128993, -0.00018089220046728499]\n",
      "600 [1.4867835316559317e-05, -5.065067796575345e-05, -2.3989843290795995e-05]\n",
      "700 [1.971765716798422e-06, -6.717270417586384e-06, -3.1815223632100903e-06]\n",
      "800 [2.6149469369030636e-07, -8.908414196052704e-07, -4.2193208287814765e-07]\n",
      "900 [3.467931014604295e-08, -1.1814299344070243e-07, -5.5956445449048146e-08]\n",
      "\n",
      "----\n",
      "\n",
      "그레이디언트의 최종 값: [9.57758165411209e-09, -3.262822016281278e-08, -1.5453808714914104e-08]\n",
      "v의 최후 위치와 최솟점 사이의 거리: 1.830234305038648e-08\n"
     ]
    }
   ],
   "source": [
    "# 임의의 지점 선택\n",
    "random.seed(42)\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "# gradient_step 1000번 반복\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    v = gradient_step(v, grad, -0.01)\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "print(\"\\n----\\n\")        \n",
    "print(f\"그레이디언트의 최종 값: {grad}\")\n",
    "print(f\"v의 최후 위치와 최솟점 사이의 거리: {distance(v, [0, 0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에포크와 스텝 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용된 코드에서 에포크(epoch)는 이동 횟수를 가리키며, \n",
    "에포크를 크게 하면 최솟값 지점에 보다 가까워진다.\n",
    "하지만 항상 수렴하는 방향으로 이동하는 것은 아니다.\n",
    "하지만 여기서는 스텝 크기를 너무 크게 지정하지만 않으면 항상 최솟값에 수렴하는 \n",
    "볼록함수만 다룬다. \n",
    "스텝 크기에 따른 수렴속도는 다음과 같다.\n",
    "\n",
    "* 스텝 크기 크게: 수렴 속도가 빨라진다.\n",
    "* 스텝 크기 작게: 수렴 속도가 느려진다.\n",
    "\n",
    "하지만 다루는 함수에 따른 적당한 스텝의 크기가 달라지며,\n",
    "보통 여러 실혐을 통해 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 이용하여 주어진 데이터들의 분포에 대한 선형 모델을 구하는 방법을\n",
    "**선형회귀**(linear regression)라 부른다. \n",
    "\n",
    "먼저 $y = f(x) = 20*x + 5$ 일차함수의 그래프에 해당하는 데이터를 구한다. \n",
    "여기서 $x$는 -0.5에서 0.5 사이에 있는 100개의 값으로 주어지며,\n",
    "$y$값에 약간의 잡음(가우시안 잡음)이 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x는 -0.5에서 0.5 사이\n",
    "xs = [x/100 for x in range(-50, 50)]\n",
    "\n",
    "# 약간의 잡음 추가 (가우시안 잡음)\n",
    "error = [random.randrange(-100,100)/100 for _ in range(-50, 50)]\n",
    "\n",
    "# y = 20*x + 5 + 가우시안 잡음\n",
    "ys = [20*x + 5 + e for x, e in zip(xs, error)]\n",
    "\n",
    "# (x,y) 좌표값들의 리스트\n",
    "inputs = list(zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 프로그램은 잡음이 포함되어 직선으로 그려지지 않는 \n",
    "데이터의 분포를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAUYElEQVR4nO3db6wc11nH8d/jTVIk/oj0uklNkovzIi+I6rbAEmoFgUtalISINFJBDRBHKtitilEtIdHcVqKV/CKhKtQgQsFuE2wVGironygylDYQ5UVd1GtAUUKARiG5hFix41ZQ3jiy/fBiduPJ3Jnd2Zkzs3Nmvh8punv3zp09kz8/nzzznDPm7gIAxGvLsgcAAKiHIAeAyBHkABA5ghwAIkeQA0DkLlnGh27dutW3b9++jI8GgGidOHHiZXd/Q/b9pQT59u3btb6+voyPBoBomdnzee9TWgGAyBHkABA5ghwAIkeQA0DkCHIAiBxBDgCRI8gBoCnHj0v33pt8bdBS+sgBoPeOH5duukl65RXpssukRx+Vdu5s5KMIcgBowmOPJSF+/nzy9bHHLr6/a1fQUC8d5Gb2gKTbJJ1y9zdN3vuYpD2STk8O+7C7Hws2OgCIzfHjSVivrCQz8emMfGWlsRn6IjPyP5P0R5KOZt7/pLt/IshoACBm2XLKwYPSmTPJDDxvht52kLv742a2PcinAkAfZcP6zBlpbe3iz9Mz9F27gn1siK6VfWb2hJk9YGaXFx1kZnvNbN3M1k+fPl10GADEa9euJKRHo81hvXNnUk45cCD4jU9b5OHLkxn5I6ka+ZWSXpbkkg5I2ubu7513nvF47Ox+CKCXpjXywDc0JcnMTrj7OPt+ra4Vd38p9QGHJT1S53wAEL2dOxtrMyxSq7RiZttS394h6cl6wwEALGqR9sPPSdolaauZvSDpo5J2mdlblZRWnpP0vgbGCACYYZGulTtz3v5MwLEAACpgrxUAiBxL9AFAKtdtkj5Gaqw7ZVEEOQCU2eAqfcxoJJlJ585tXsG5hFAnyAGgzPL59DEXLiTvuUtnz0r79iXvNbzLYRFq5AAwa0Vm3jGXXnrx9ZYtSbhndzlsETNyAJgun59V884eI13c5XD//kb2UClroSX6obBEH0CvNLgsP62RJfoAELVQAbyEZflpBDmAYWrxUWxN42YngP7Lewhy0aPYIsSMHEC/Fc28p10oeTcpW6p5h0KQA+inaRhvbOT3iBd1qsx6XFtHQ50gB9A/2VWYl0yiLu+pPbMW/nRgsU8ZBDmA/kmHsSTt2SOtrpabVadLLmYXV3IGfmBySAQ5gHgV1bKz9e/du8sHcLrk0oHFPmUQ5ADiMg3vbMimyx5lVmrOki657NjR+RufBDmA7ssLb7Ok5FFU9gi1SGfJi33KIMgBdFv6xmU6vLdsubidbIfLHm0gyAF0W/rGZTa8q7QGRtYjXgZBDqDbsjcu6/R192hZfhpBDqDb6t64TCvzAIkIEeQAuqOo7BHqhuOsZfkRI8gBdEMbZY+Qs/sOIcgBdENbZY8I2gkXxTa2ALqhzHMzkYsZOYBu6GnZow0EOYDuqFv26GGPeBkEOYB+6GmPeBnUyAH0Q48e3bYoghxAN+U9Z3OWAd8spbQCoHuqlEkGfLOUIAfQPVV7ynvYI14GpRUA3TPgMkkVzMgB1Ldo29+84wdcJqmCIAdQT9l6dtEj2oq2pR1omaQKghxAPWXq2UVP+Tl7Vtq3L3k9sN7vkKiRA6inTD07HfYXLiTHjkbJE3/Onx9k73dIzMgB1FOmnl30lJ9smYWbmpWUDnIze0DSbZJOufubJu+9XtJfStou6TlJv+Tu3wk/TACdlq5n593InBX2O3ZwU7Mmc/dyB5r9tKT/k3Q0FeQfl/Rtd7/PzO6RdLm7f2jeucbjsa+vr9cYNoBOGvB+J20wsxPuPs6+X7pG7u6PS/p25u3bJR2ZvD4i6V2VRwggLnlL6Ae838ky1a2RX+nuJyXJ3U+a2RVFB5rZXkl7JWl1dbXmxwJYqqKZd7YWvrKShD1lk0a1drPT3Q9JOiQlpZW2PhdAA4paDtO18OyNTMosjanbfviSmW2TpMnXU/WHBKDzZrUc7twpra0lXSmUWVpRd0b+sKS7Jd03+frl2iMC0H1VWg5pLWzMIl0rn5O0S9JWSS9J+qikL0n6vKRVSRuSftHdszdEN6FrBWhR3cef1fn9gT56rSlFXSulZ+TufmfBj26qPCoAzarbDpj9/aJ9UYqwX0orWNkJ9FnVfb3zfp99UTqLvVaAPqu7r3f699kXpbOYkQN9Vndf71nthNy87IzSNztD4mYnECluXi5V7ZudAMDNy26iRg70Rd7eJxgEZuRAH4RoM6RkEi2CHOiDOm2GdXvFsXQEOdAHZZfD58286RWPHkEOxGBe6aNMm2GZrWfNLj5Xs8oCIiwFQQ50Xdn697yOkipbz9IrHgWCHOi6usvsp+Y99GF6Tp6hGR2CHOi6UNvBln3oA73i0aGPHOi6aQAfOFD+5mNRTzkPfeglZuRADBaZJZepqfPQh14hyIG+KVNTr7uZFjqFIAe6JMQKy7KzbWrhvUGQA11Rd5n9FLPtwSHIga4I1WYoMdseGLpWgLYVdZTUfZoPBosZOdCmWeUTSiKoiCAH2jC9ibmxMbt8QkkEFRDkQNPSs/DRSLpk8p9diPIJ+4hDBDnQvPRNTEnas0daXb0YvouG8fT4WcvsMSgEOdC0bF/37t0XA3dWzTwv4NPHmyXbzbLl7OAR5EDTZt3ELGo5LAr49PFbtiSlGjO6XAaOIAdCKiqTFN3ELFqFWRTw2eN5LBtEkAPhVFmZWTRbLwp4WhSRgyAHQqm6MjNvtj4rsGlRRAZBDoQSemtYAhslEeRAKJQ9sCQEORBSehbNYh20hCAHmhBqS1qgBHY/BKTiHQmryrvxCTSEGTmw6OrKWe9P8UxMtIggBxZdXVmmbMKNT7SIIAcWXV1Ztl+c9kG0hCAHFl1dSdkEHWPuXv8kZs9J+q6k85LOuft41vHj8djX19drfy7QiHT9W6pWIwcaYGYn8vI15Iz87e7+csDzAe3Lq3+vrW0+jrIJOoT2QyCtSttg6NZFYEGhZuQu6e/MzCX9qbsfCnReoF2L1r9Z+IMOCBXkN7r7i2Z2haSvmtm/ufvj6QPMbK+kvZK0uroa6GOBEhapZ89qG8w7T9UdD4GAggS5u784+XrKzL4o6QZJj2eOOSTpkJTc7AzxucBcVfcIzx5TdB46WNABtWvkZva9Zvb909eSfk7Sk3XPCwQRaql80XmmM/gDByirYGlCzMivlPRFM5ue7y/c/W8DnBeoL9SMedZ56GDBktUOcnd/VtJbAowFCC/UUnmW3KPDgiwIWhQLggBgcUULgugjx7DQ840eYq8VDAc93+gpZuTop7yZNw97QE8xI0f/0PONgWFGjn5Iz8DL9nxL1MvRC8zIsVwhtoPNzsAPHpzf8029HD1CkGN5QoVpdgZ+5sz8nm/2SEGPEORYniphmjeDz6t9z1ttSb0cPUKQY3lCbRlbZdUlKzXRIwQ5lmfRMJ01g6+y3wl7pKAnCHIs1yJhSjkEyEWQIx6UQ4BcBDnikp7B8yR7QBJBjljRBw68ipWd6I5FdiZk3xTgVczI0Q2LzrC58Qm8ihk5umHWDDtvps6zMoFXMSNHNxTNsGfN1OkDByQR5KgqdMdItrVQSmbhGxvsiQLMQZBjcU11jOTtTDgaSZdM/jWlFg7kIsixuKZ3DkyfX5L27JFWV+kXBwoQ5ChWVD6p2zEyryyTPf/u3QQ4MANBjnzzbjJWXSpfpizDUnxgIQQ58s0rnxQtlZ/+bvb19NiyZRk6UoDSCHLkK1s+yd6YNJPOnXvt6+nj186ckVZWWMgDBEaQI1/Z8kZ6hn3hQvKe+2tfnz0r7duXvJcOdcomQBAEOYqVKW+kZ+5FM3Kzi0E/fabm2lorlwAMAUGOevIW8mRfr6xI+/dTTgEaYu7e+oeOx2NfX19v/XOxROwdDtRmZifcfZx9nxk52kEXCtAYdj8EgMgR5EO1yEMcAHQapZUh4jFpQK8wIx+iKo9JYwYPdBYz8iFadNMrZvBApxHkQ7ToplRNb1sLoBaCfEiyvdx5m16V2VaWBT1ApxDkMVtkkU1ReYRtZYHoBQlyM7tZ0h9IGkn6tLvfF+K8mGHRunVReYRtZYHo1e5aMbORpPsl3SLpekl3mtn1dc+LObIBfPTo7K6SaXlkNHpteaTofQDRCDEjv0HSM+7+rCSZ2UOSbpf0rwHOjaxpOSW9r/doJD344Oa9v9NlkKLyCGUTIHq1N80ys3dLutndf33y/V2SftLd92WO2ytpryStrq7++PPPP1/rcwclHd7pXQSngb2xIR0+nMzOt2xJgn269/eirYJsbgV0VpObZlnOe5v+dHD3Q5IOScnuhwE+dxjStXCzJKCz+3ofPy4dOXLxmPTe34u0CtIvDkQpxMrOFyRdk/r+akkvBjgvpM1P4BmNNtezp+WRAwek+++XXve6i8esrJRfkVllxSeApQsxI/+mpOvM7FpJ/y3pPZJ+OcB5IW3u4S56TFq6q2THjvxSzLwZNv3iQJRqB7m7nzOzfZK+oqT98AF3f6r2yJCocjNyGur33rvYikxufAJRCtJH7u7HJB0LcS7kqNrDXWWGTb84EB1WdvYZM2xgEAjyLmmi9Y8ZNtB7BHlX0PoHoCIeLNEVtP4BqIggb1vRk3bY8wRARZRW2jSrfMKNSQAVEeRtmrdlLDcmAVRAaaVNlE8ANIAZeRvSbYVlyyfsQgigJIK8aXl18bW1xX+HMAdQgNJK06q0FdKKCGABBHnTqtTFqaUDWACllaZV3b2QVkQAJdV+1FsV4/HY19fXW/9cAIhZ0aPeKK0AQOQIcgCIHEG+TEX7rgDAArjZ2ZR5C3roFQcQCEHehDIhPW/fFQAoiSAPaToL39iYH9I8sR5AIAR5VtU9TtKz8NFIumTyt7YopOkVBxAIQZ5Wp26dLpVI0p490urq7JBm21oAAQwryOfNtrN166NHy8+Ys6WS3bsJaQCtGE6Ql5ltp8N4NJIefFA6d67c7JxSCYAlGU6Ql+kSSYfxxoZ0+PD8rpLsLJ8AB9Cyfgd5OmSzpY+VlWQxTnb2PA3j48elI0dmd5XQCw6gA/ob5HkhO51tr6xI+/fPDuAypRJ6wQF0QD+W6OctdS8K2bU16cyZcg9umB5fFM7sGw6gA+KfkReVN2YtuKm7GKfKMzgBoCHxB3nRzHtWaaRKh8k0vPPKMvOewQkADYo/yGfNrmd1kSzSYZKe9ZtJFy4kf1EXB9AB8Qd5G/3b6Vn/li1JTdyMujiATog/yKXm+7ezs/6DB5MbptTFAXRAP4K8aazaBNBhBLlUbsdDVm0C6CiCnNWZACLXjwVBdeS1LwJARAhyVmcCiFyt0oqZfUzSHkmnJ2992N2P1R1Uq7iRCSByIWrkn3T3TwQ4z/JwIxNAxCitAEDkQgT5PjN7wsweMLPLA5yvnrydEAGgx+aWVszsa5LemPOjj0j6lKQDknzy9fckvbfgPHsl7ZWk1dXVisOdg1ZCAAM0N8jd/R1lTmRmhyU9MuM8hyQdkqTxeOxlB7gQHvQAYIDqdq1sc/eTk2/vkPRk/SHNkF6BKW3uNKm7zzgARKhu18rHzeytSkorz0l6X+0RFUmXTaa7D2afcE8rIYABqhXk7n5XqIHMlS6bXLgwHcDmEgqthAAGJp69VtJlk+yMnBIKgAGLJ8izZROJEgoAKKYglzaXTQhwAGBlJwDEjiAHgMgR5AAQOYIcACJHkANA5AhyAIicuTezf9XMDzU7Len51j+4vq2SXl72IFo2xGuWhnndQ7xmKa7r/mF3f0P2zaUEeazMbN3dx8seR5uGeM3SMK97iNcs9eO6Ka0AQOQIcgCIHEG+mEPLHsASDPGapWFe9xCvWerBdVMjB4DIMSMHgMgR5AAQOYJ8BjN7vZl91cy+Nfl6+YxjR2b2z2ZW+ADqGJS5ZjO7xsz+wcyeNrOnzOyDyxhrCGZ2s5n9u5k9Y2b35PzczOwPJz9/wsx+bBnjDKnENf/K5FqfMLOvm9lbljHOkOZdc+q4nzCz82b27jbHVxdBPts9kh519+skPTr5vsgHJT3dyqiaVeaaz0n6LXf/EUlvk/QbZnZ9i2MMwsxGku6XdIuk6yXdmXMdt0i6bvLXXkmfanWQgZW85v+U9DPu/mZJBxT5zcCS1zw97nclfaXdEdZHkM92u6Qjk9dHJL0r7yAzu1rSz0v6dEvjatLca3b3k+7+T5PX31XyB9hVrY0wnBskPePuz7r7K5IeUnL9abdLOuqJb0j6QTPb1vZAA5p7ze7+dXf/zuTbb0i6uuUxhlbmn7Mk/aakv5Z0qs3BhUCQz3alu5+UkvCSdEXBcQcl/bakC20NrEFlr1mSZGbbJf2opH9sfGThXSXpv1Lfv6DNfyCVOSYmi17Pr0n6m0ZH1Ly512xmV0m6Q9KftDiuYOJ61FsDzOxrkt6Y86OPlPz92ySdcvcTZrYr5NiaUveaU+f5PiUzmP3u/r8hxtYyy3kv249b5piYlL4eM3u7kiD/qUZH1Lwy13xQ0ofc/bxZ3uHdNvggd/d3FP3MzF4ys23ufnLyv9N5/8t1o6RfMLNbJX2PpB8ws8+6+682NOTaAlyzzOxSJSH+5+7+hYaG2rQXJF2T+v5qSS9WOCYmpa7HzN6spFR4i7ufaWlsTSlzzWNJD01CfKukW83snLt/qZ0h1kNpZbaHJd09eX23pC9nD3D3NXe/2t23S3qPpL/vcoiXMPeaLfm3/TOSnnb3329xbKF9U9J1ZnatmV2m5J/fw5ljHpa0e9K98jZJ/zMtPUVq7jWb2aqkL0i6y93/YwljDG3uNbv7te6+ffLf8V9J+kAsIS4R5PPcJ+mdZvYtSe+cfC8z+yEzO7bUkTWnzDXfKOkuST9rZv8y+evW5Qy3Onc/J2mfki6FpyV93t2fMrP3m9n7J4cdk/SspGckHZb0gaUMNpCS1/w7klYk/fHkn+36koYbRMlrjhpL9AEgcszIASByBDkARI4gB4DIEeQAEDmCHAAiR5ADQOQIcgCI3P8DXgZX7rIZVGsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 그래프를 선형적으로 묘사하는 함수를 경사하강법을 이용하여 구현한다.\n",
    "구현 대상은 아래 모양의 함수이다. \n",
    "\n",
    "$$\n",
    "\\hat y = f(x) = \\theta_0 x + \\theta_1\n",
    "$$\n",
    "\n",
    "* $\\theta_0$: 직선의 기울기\n",
    "* $\\theta_1$: 절편\n",
    "* $\\hat y$: $y$에 대한 예측치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프를 가장 잘 묘사하는 직선을 구해야 한다.\n",
    "즉, 적절한 $\\theta_0$와 $\\theta_1$을 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 적용하려면 최소화 대상함수를 정해야 한다.\n",
    "여기서는 예측치 $\\hat y$와 실제 $y$ 사이의 오차의 **평균제곱오차**(mean squared error, MSE)를\n",
    "계산하는 함수를 사용한다.\n",
    "\n",
    "$$\n",
    "\\textrm{MSE}(\\theta_0, \\theta_1) = \\sum_{y \\in ys} (\\hat y - y)^2\n",
    "= \\sum_{x \\in \\textrm{xs}} (\\theta_0 x + \\theta_1 - y)^2\n",
    "$$\n",
    "\n",
    "즉, MSE를 최소로 하는 $\\theta_0, \\theta_1$을 구해야 한다.\n",
    "MSE의 그레이디언트는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y) x\\, ,\n",
    "\\qquad\n",
    "\\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y)\n",
    "$$\n",
    "\n",
    "아래 코드는 특정 x에 대한 실제 값 y와 예측치 $\\hat y$ 사이의 제곱오차와\n",
    "제곱오차의 그레이디언트를 계산한다.\n",
    "\n",
    "* `slope`: $\\theta_0$ (기울기)\n",
    "* `intercept`: $\\theta_1$ (절편)\n",
    "* `predicted`: $\\hat y$\n",
    "* `error`: $(\\hat y - y)$\n",
    "* `grad`: $(2(\\hat y - y) x, 2(\\hat y - y))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    # 기울기와 절편\n",
    "    slope, intercept = theta\n",
    "    # 예측치\n",
    "    predicted = slope * x + intercept\n",
    "    # 오차\n",
    "    error = (predicted - y)          \n",
    "    # 제곱 오차\n",
    "    squared_error = error ** 2       \n",
    "    # 특정 x에 대한 제곱오차의 그레이디언트 항목\n",
    "    grad = [2 * error * x, 2 * error]\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 $\\theta = (\\theta_0, \\theta_1)$로 시작한 후에\n",
    "경사하강법으로 평균제곱오차의 최솟값 지점을 구할 수 있다.\n",
    "\n",
    "* `vector_mean`: 벡터 항목들의 평균값 계산\n",
    "* `epoch`(에포크): 5000회 반복\n",
    "* `learning_rate`: 스텝 크기. 보통 **학습률**이라 부름.\n",
    "\n",
    "아래 코드에서 사용한 학습률은 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.5573631781037754, -0.3409606698035382]\n",
      "500 [1.0860605634068832, 2.9677764519232985]\n",
      "1000 [2.6080336786406573, 4.18874863946214]\n",
      "1500 [4.011976376077862, 4.642064339793808]\n",
      "2000 [5.305013852497387, 4.812897015495151]\n",
      "2500 [6.495163711496766, 4.879577951303121]\n",
      "3000 [7.5903398484398314, 4.90767120517886]\n",
      "3500 [8.598020889846392, 4.921296576290668]\n",
      "4000 [9.525160097963871, 4.929341055389218]\n",
      "4500 [10.378181475323187, 4.9350917239397925]\n",
      "최종 기울기: 11.161\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 최종 기울기가 11.398로 애초에 사용한 20과 차이가 크다.\n",
    "이유는 학습률이 너무 낮아서 5000번의 에포크가 충북한 학습을 위해 부족했기 때문이다.\n",
    "이에 대한 해결책은 보통 두 가지이다. \n",
    "\n",
    "* 첫째: 학습률 키우기\n",
    "* 둘째: 에포크 키우기\n",
    "\n",
    "아래 코드는 먼저 에포크를 네 배 늘린 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.539845326196926, -0.8765658904200774]\n",
      "1000 [3.5347058588398195, 4.120657569741477]\n",
      "2000 [6.089146831484904, 4.807295221174362]\n",
      "3000 [8.254060613242245, 4.909957251324375]\n",
      "4000 [10.08698653375398, 4.932225333575831]\n",
      "5000 [11.63858380675949, 4.942345701376079]\n",
      "6000 [12.951998593477466, 4.949733516900191]\n",
      "7000 [14.063789219509484, 4.955827987429306]\n",
      "8000 [15.0049066769235, 4.960965378519356]\n",
      "9000 [15.801551256517898, 4.965311213690506]\n",
      "10000 [16.475901275838954, 4.968989518433547]\n",
      "11000 [17.046730424738943, 4.972103106006536]\n",
      "12000 [17.529930406587045, 4.974738713135767]\n",
      "13000 [17.938953356610103, 4.976969721779518]\n",
      "14000 [18.285186344872766, 4.978858243526626]\n",
      "15000 [18.57826838869039, 4.980456854364114]\n",
      "16000 [18.826358799830466, 4.981810059132508]\n",
      "17000 [19.036364337180647, 4.982955530628767]\n",
      "18000 [19.21413148874078, 4.983925158418778]\n",
      "19000 [19.36460923600656, 4.984745936634814]\n",
      "최종 기울기: 19.492\n",
      "최종 절편: 4.985\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(20000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 1000번에 한 번 학습과정 확인\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 아래 코드는 학습률을 0.01로 키웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.6757583873253522, 0.6957201300303815]\n",
      "500 [11.706690051576645, 4.942804228398626]\n",
      "1000 [16.50780846999213, 4.969163560245711]\n",
      "1500 [18.593137756203685, 4.98053795906695]\n",
      "2000 [19.498884502023184, 4.9854783387029045]\n",
      "2500 [19.89228863870891, 4.98762415462158]\n",
      "3000 [20.0631607014304, 4.988556173271237]\n",
      "3500 [20.137377668401463, 4.988960988407408]\n",
      "4000 [20.16961323750889, 4.9891368167500385]\n",
      "4500 [20.18361450915995, 4.9892131864390965]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교했을 때 학습률을 키우는 것이 보다 효과적이다. \n",
    "최종적으로 구해진 기울기와 절편을 이용하여 처음에 주어진 데이터의 분포를\n",
    "선형적으로 학습한 1차함수의 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측치\n",
    "zs = [slope *x + intercept for x in xs]\n",
    "\n",
    "# 실제 데이터 분포\n",
    "plt.plot(xs, ys, 'r.', label='Actuals')\n",
    "# 예측치 그래프\n",
    "plt.plot(xs, zs, 'b-', label='Estimates')\n",
    "\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 4: 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 경사하강법은 주어진 데이터셋 전체를 대상으로 평균제곱오차와 그레이디언트를 계산하였다.\n",
    "이런 방식을 **배치 경사하강법**이라 부른다.\n",
    "\n",
    "**주의**: 배치(batch)는 원래 하나의 묶음을 나타내지만 여기서는 주어진 (훈련) 데이터셋 전체를 가리키는\n",
    "의미로 사용된다.\n",
    "\n",
    "그런데 사용된 데이터셋의 크기가 100이었기 때문에 계산이 별로 오래 걸리지 않았지만,\n",
    "데이터셋이 커지면 그러한 계산이 매우 오래 걸릴 수 있다.\n",
    "실전에서 사용되는 데이터셋의 크기는 몇 만에서 수십억까지 다양하며, \n",
    "그런 경우에 적절한 학습률을 찾는 과정이 매우 오래 걸릴 수 있다.\n",
    "\n",
    "데이터셋이 매우 큰 경우에는 따라서 아래 두 가지 방식을 추천한다.\n",
    "\n",
    "* 미니배치 경사하강법\n",
    "* 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 경사하강법과는 달리 미니매치 경사하강법(mini-batch gradient descent)은\n",
    "일정 크기의 훈련 데이터를 대상으로 그레이디언트를 계산한다.\n",
    "\n",
    "예를 들어, 전체 데이터셋의 크기가 1000이고 미니배치의 크기를 10이라 하면,\n",
    "배치 경사하강법에서는 하나의 에포크를 돌 때마다 한 번 MSE와 그레이디언트를 계산하였지만\n",
    "미니배치 경사하강법에서는 10개의 데이터를 확인할 때마다 MSE와 그레이디언트를 계산하여\n",
    "하나의 에포크를 돌 때마다 총 100번 기울기와 절편을 업데이트한다. \n",
    "\n",
    "아래 코드에서 정의된 `minibatches()` 함수는 호출될 때마다\n",
    "`batch_size`로 지정된 크기의 데이터 세트를 전체 데이터셋에서\n",
    "선택해서 내준다.\n",
    "\n",
    "데이터를 선택하는 방식은 다음과 같다.\n",
    "\n",
    "* `minibatches()` 함수는 제너레이터이다. \n",
    "    즉, 요구될 때마다 항목을 생성하여 리턴하는 함수이다. \n",
    "* 전체 데이터셋을 인덱스 기준으로 미니배치 크기(`batch_size`) 만큼씩 구분하여\n",
    "    `batch_starts` 리스트에 저장한다.\n",
    "* 섞기(`shuffle`) 옵션이 `True`일 경우, `batch_starts`에 포함된 항목들의 순서를 무작위로 섞는다.\n",
    "* 최종적으로 `batch_starts` 에 포함된 인데스를 기준으로 지정된 미니배치 크기만큼의 \n",
    "    데이터를 다음 MSE, 그레이디언트 계산용으로 내어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "\n",
    "# 제너레이터 함수 정의\n",
    "def minibatches(dataset: List[float],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[float]]:\n",
    "    \"\"\"\n",
    "    dataset: 전체 데이터셋\n",
    "    batch_size: 미니배치 크기\n",
    "    shuffle: 섞기 옵션\n",
    "    리턴값: 이터레이터\n",
    "    \"\"\"\n",
    "\n",
    "    # 0번 인덱스부터 시작하여, batch_size 배수 번째에 해당하는 인덱스만 선택\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    # shuffle 옵션이 참이면 인덱스 섞기\n",
    "    if shuffle: random.shuffle(batch_starts)\n",
    "\n",
    "    # batch_starts에  포함된 인덱스를 기준으로 해서 미니배치 크기만큼씩 선택해서 \n",
    "    # 다음 MSE와 그레이디언트 계산에 필요한 훈련 데이터 세트를 지정함.\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 미니배치 경사하강법을 이전에 사용했던 데이터에 대해 적용한다.\n",
    "학습률(`learning_rate`)을 0.001로 하면 학습이 제대로 이루어지지 않는다.\n",
    "에포크 수를 키우거나 합습률을 크게 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.18096617572366355, -0.810253155496382]\n",
      "100 [1.4325823295978932, 2.7965832049687727]\n",
      "200 [2.9277291181808662, 4.127583024520555]\n",
      "300 [4.307256114408177, 4.6201205624228345]\n",
      "400 [5.577858834315078, 4.805372329280058]\n",
      "500 [6.747297927532496, 4.877973082222024]\n",
      "600 [7.823358363323769, 4.9075106536945095]\n",
      "700 [8.813363542511828, 4.9227014341159645]\n",
      "800 [9.724173605813512, 4.930098309332715]\n",
      "900 [10.562099366999867, 4.935898279024058]\n",
      "최종 기울기: 11.326\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 키우면 좋은 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.0089547406154515, 0.9789387260040882]\n",
      "100 [11.920008658960944, 4.952894140709578]\n",
      "200 [16.630466145513115, 4.970879458498564]\n",
      "300 [18.65959750842576, 4.983489508744]\n",
      "400 [19.533544758414724, 4.986114887570471]\n",
      "500 [19.910159234650248, 4.9876175552247615]\n",
      "600 [20.072120246973697, 4.988277830931272]\n",
      "700 [20.141813699267466, 4.989044796751943]\n",
      "800 [20.171950036505514, 4.989673862726055]\n",
      "900 [20.18490980813245, 4.988704425280094]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 두고 에포크를 3000으로 늘려도 성능이 그렇게 좋아지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.818987279044814, 0.5496759496355055]\n",
      "100 [11.132254675200233, 4.944402547449223]\n",
      "200 [16.29128890751974, 4.967275599762484]\n",
      "300 [18.51315652813586, 4.978364613062517]\n",
      "400 [19.470399111844596, 4.984337308055089]\n",
      "500 [19.882668827278895, 4.988157388454654]\n",
      "600 [20.060140900004065, 4.987966154755475]\n",
      "700 [20.136885552244106, 4.987828522542775]\n",
      "800 [20.16977510712028, 4.988976100905835]\n",
      "900 [20.183962972393914, 4.989098169073801]\n",
      "1000 [20.19002718033645, 4.989306092251666]\n",
      "1100 [20.19268900836928, 4.989013649684174]\n",
      "1200 [20.193941008879644, 4.98931327368746]\n",
      "1300 [20.194440751083505, 4.988662935916931]\n",
      "1400 [20.194676502572356, 4.989409414992472]\n",
      "1500 [20.19501205890768, 4.989279358430207]\n",
      "1600 [20.194844639671665, 4.9895094830280255]\n",
      "1700 [20.195038900604747, 4.989915517219994]\n",
      "1800 [20.195104991373057, 4.988570101328778]\n",
      "1900 [20.195111383724566, 4.989401145459436]\n",
      "2000 [20.194820820191822, 4.988704721391355]\n",
      "2100 [20.194956704659234, 4.989706094313501]\n",
      "2200 [20.194893907702024, 4.989209975413154]\n",
      "2300 [20.19482708931829, 4.989765252131663]\n",
      "2400 [20.19497162191007, 4.990361301343332]\n",
      "2500 [20.19504959681753, 4.989715152995273]\n",
      "2600 [20.195037903439133, 4.989885179868943]\n",
      "2700 [20.195048938410093, 4.989683544147567]\n",
      "2800 [20.194794217945287, 4.988956630961846]\n",
      "2900 [20.19472935840392, 4.9896646884885545]\n",
      "최종 기울기: 20.194\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 3000번의 에포크\n",
    "for epoch in range(3000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정을 살펴보면 기울기가 19.966 정도에서 더 이상 좋아지지 않는다.\n",
    "이런 경우 특별히 더 좋은 결과를 얻을 수 없다는 것을 의미한다.\n",
    "사실, 데이터셋을 지정할 때 가우시안 잡음을 추가하였기에 완벽한 선형함수를 찾는 것은 애초부터 불가능하다.\n",
    "따라서 위 결과를 미니배치 경사하강법을 사용한 선형회귀로 얻을 수 있는 최선으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률적 경사하강법(stochastic gradient descent, SGD)은\n",
    "미니배치의 크기가 1인 미니배치 경사하강법을 가리킨다.\n",
    "즉, 하나의 데이터를 학습할 때마다 그레이디언트를 계산하여 기울기와 절편을 업데이트 한다. \n",
    "\n",
    "아래 코드는 주어진 데이터셋을 대상로 SGD를 적용하는 방식을 보여준다.\n",
    "학습률을 0.001로 했음에도 불구하여 1000번의 에포크를 반복한 후에 이전에\n",
    "0.01을 사용했던 경우와 거의 동일한 결과를 얻는다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.15554810709612274, 1.509287732512692]\n",
      "100 [16.342595331414632, 5.034998314265212]\n",
      "200 [19.472338990801568, 4.997992330602081]\n",
      "300 [20.06303054508488, 4.991008012136133]\n",
      "400 [20.174514599453477, 4.989689828163571]\n",
      "500 [20.19555552053715, 4.989441040970666]\n",
      "600 [20.199526674641238, 4.989394086168721]\n",
      "700 [20.200276169630307, 4.98938522416341]\n",
      "800 [20.200417625419696, 4.9893835515945835]\n",
      "900 [20.200444323050316, 4.989383235922631]\n",
      "최종 기울기: 20.200\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 하면 오히려 결과가 나빠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.259915006214208, 6.8246196207535865]\n",
      "100 [20.261030045940156, 4.990193171642614]\n",
      "200 [20.26103222379059, 4.990192825899716]\n",
      "300 [20.26103222379092, 4.990192825899663]\n",
      "400 [20.26103222379092, 4.990192825899663]\n",
      "500 [20.26103222379092, 4.990192825899663]\n",
      "600 [20.26103222379092, 4.990192825899663]\n",
      "700 [20.26103222379092, 4.990192825899663]\n",
      "800 [20.26103222379092, 4.990192825899663]\n",
      "900 [20.26103222379092, 4.990192825899663]\n",
      "최종 기울기: 20.261\n",
      "최종 절편: 4.990\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 것에 따르면 확률적 경사하강법의 성능이 가장 좋았다.\n",
    "하지만 이것은 경우에 따라 다르다.\n",
    "배치 경사하강법, 미니배치 경사하강법, 확률적 경사하강법 각각의 장단점이 있지만\n",
    "여기서는 더 이상 자세히 다루지 않는다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
