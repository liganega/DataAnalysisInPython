{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법과 선형회귀\n",
    "1. 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 모듈 불러오기\n",
    "\n",
    "이전에 정의한 함수를 사용하기 위한 준비가 필요하며,\n",
    "이전에 사용한 모든 파이썬 코드가 `../scratch/` 디렉토리에 저장되어 있다고 가정한다.\n",
    "\n",
    "여기서는 선형대수 모듈에 포함되어 있는 `Vector` 자료형과 `dot` (벡터곱)함수를 불러온다.\n",
    "\n",
    "* `Vector = List[float]`\n",
    "* `dot(v:Vector, w:Vector) -> float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 선형대수 모듈로부터 Vector 자료형과 dot 함수 불러오기\n",
    "from scratch.linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 1: 경사하강법 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$의 최댓값(최솟값)을 구하고자 한다.\n",
    "예를 들어, \n",
    "길이가 $n$인 실수 벡터를 입력받아 항목들의 제곱의 합을 계산하는 함수가 다음과 같다고 하자.\n",
    "\n",
    "$$\n",
    "f(\\mathbf x) = f(x_1, ..., x_n) = \\sum_{k=1}^{n} x_k^2 = x_1^2 + \\cdots x_n^2\n",
    "$$\n",
    "\n",
    "아래 코드에서 정의된 `sum_of_squares()`가 위 함수를 파이썬으로 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최댓값(최솟값)이 존재한다는 것이 알려졌다 하더라도 실제로 최댓값(최솟값)\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트를 방향(반대 방향)으로 조금씩 이동하면서 최댓값(최솟값) 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$가 vector $\\textbf{x}\\in \\textbf{R}$에서 \n",
    "미분 가능할 때 그레이디언트는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial x_n} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $(\\frac{\\partial}{\\partial x_1} f(\\textbf{x}), \\frac{\\partial}{\\partial x_2} f(\\textbf{x}))$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../images/tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 다음 과정을 반복하여 최댓값(최솟값) 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. \n",
    "\n",
    "최솟값 지점에 가까워질 수록 그레이디언트는 점점 0벡터에 가까워진다. \n",
    "따라서 그레이디언트가 충분히 작아지면 최솟값 지점에 매우 가깞다고 판단하여 그 위치에서\n",
    "최솟값의 근사치를 구하여 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 2: 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_h \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 `square()`의 도함수 $f'(x) = 2x$는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 많지 않다.\n",
    "따라서 여기서는 많은 경우 충분히 작은 $h$에 대한 함수의 변화율을 측정하면\n",
    "미분값의 근사치를 사용할 수 있다는 사실을 확인하고자 한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `derivative()` 를 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 그래프로 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "# 실제 도함수 그래프(빨간색 점)\n",
    "plt.plot(xs, actuals, 'r.', label='Actual') \n",
    "# 근사치 그래프(검은색 +)\n",
    "plt.plot(xs, estimates, 'k+', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{x}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}f(\\mathbf x) = \\lim_h \\frac{f(\\mathbf{x}_h) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{x}_h$는 $\\mathbf x$의 $i$번째 항목에 $h$를 더한 벡터를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial x_i} f(\\mathbf x)$ 는 $x_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial x_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다.\n",
    "\n",
    "아래 코드에서 정의된 `partial_difference_quotient`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 `i`번째 편도함수의 근사치를 지정하기 위해 `i`번째 매개변수에만 `h`가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                v: Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 `h`의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "벡터 `v`의 길이가 $n$이면 `estimate_gradient()` 함수를 호출할 때마다\n",
    "`f`를 $2n$ 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 3: 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 제곱함수 `sum_of_squares()` 는 `v`가 0 벡터일 때 가장 작은 값을 갖는다. \n",
    "이 사실을 경사하강법을 이용하여 확인해보자. \n",
    "\n",
    "먼저, `sum_of_squares()` 함수의 그레이디언트는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\frac{\\partial}{\\partial x_2} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래 코드는 리스트를 이용하여 그레이디언트를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 지점(`v`)에서 계산된 그레이디언트에 스텝(step)이라는 특정 상수를 곱한 값을\n",
    "더해 새로운 지점을 계산하는 함수를 구현한다.\n",
    "\n",
    "* `add`, `scalar_multiply`, `distance` 등은 선형대수 부분에서 정의한 `linear_algebra` 모듈에서 가져온다.\n",
    "* `add(v, step)`: 스텝사이즈가 음수이면 그레이디언트가 가리키는 방향의 반대방향으로 지정된 비율만큼 움직인 벡터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scratch.linear_algebra import distance, add, scalar_multiply\n",
    "\n",
    "# v에서의 그레이디언트를 구한 후 스텝이 지정한 크기 비율과 방향으로 이동한 새로운 벡터 v'을 계산한다.\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 지점에서 출발하여 `gradient_step`을 충분히 반복하면\n",
    "제곱함수의 최솟점에 충분히 가깝게 근사할 수 있음을 아래 코드가 보여준다.\n",
    "실제로 1.0e-07보다 작다.\n",
    "\n",
    "* `random.seed(42)`: 실행할 때마다 동일한 결과를 보장해준다. 사용하지 않으면 매번 다른 결과가 나옴.\n",
    "* `grad`: 이동할 때마다 계산된 그레이디언트. 최종적으로 0 벡터에 가까운 값을 갖게 됨.\n",
    "* `if epoch%100 == 0`: 위치 이동을 100번 할 때마다 현재 위치 확인\n",
    "* `epoch`(에포크): 여기서는 그레이디언트 계산 횟수. 즉, 이동횟수를 가리킴.\n",
    "* `step_size=-0.01`: 그레이디언트 반대 방향, 즉, 최솟값 지점을 향해 이동할 때 사용되는 크기 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.732765249774521, -9.309789197635727, -4.409425359965263]\n",
      "100 [0.3624181137897112, -1.2346601088642208, -0.5847760329896551]\n",
      "200 [0.048063729299005625, -0.16374007531854073, -0.07755273779298358]\n",
      "300 [0.006374190434279768, -0.02171513607091833, -0.010285009644527733]\n",
      "400 [0.0008453423045827667, -0.002879851701919325, -0.0013639934114305214]\n",
      "500 [0.00011210892101281368, -0.00038192465375128993, -0.00018089220046728499]\n",
      "600 [1.4867835316559317e-05, -5.065067796575345e-05, -2.3989843290795995e-05]\n",
      "700 [1.971765716798422e-06, -6.717270417586384e-06, -3.1815223632100903e-06]\n",
      "800 [2.6149469369030636e-07, -8.908414196052704e-07, -4.2193208287814765e-07]\n",
      "900 [3.467931014604295e-08, -1.1814299344070243e-07, -5.5956445449048146e-08]\n",
      "\n",
      "----\n",
      "\n",
      "그레이디언트의 최종 값: [9.57758165411209e-09, -3.262822016281278e-08, -1.5453808714914104e-08]\n",
      "v의 최후 위치와 최솟점 사이의 거리: 1.830234305038648e-08\n"
     ]
    }
   ],
   "source": [
    "# 임의의 지점 선택\n",
    "random.seed(42)\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "# gradient_step 1000번 반복\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    v = gradient_step(v, grad, -0.01)\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "print(\"\\n----\\n\")        \n",
    "print(f\"그레이디언트의 최종 값: {grad}\")\n",
    "print(f\"v의 최후 위치와 최솟점 사이의 거리: {distance(v, [0, 0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에포크와 스텝 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용된 코드에서 에포크(epoch)는 이동 횟수를 가리키며, \n",
    "에포크를 크게 하면 최솟값 지점에 보다 가까워진다.\n",
    "하지만 항상 수렴하는 방향으로 이동하는 것은 아니다.\n",
    "하지만 여기서는 스텝 크기를 너무 크게 지정하지만 않으면 항상 최솟값에 수렴하는 \n",
    "볼록함수만 다룬다. \n",
    "스텝 크기에 따른 수렴속도는 다음과 같다.\n",
    "\n",
    "* 스텝 크기 크게: 수렴 속도가 빨라진다.\n",
    "* 스텝 크기 작게: 수렴 속도가 느려진다.\n",
    "\n",
    "하지만 다루는 함수에 따른 적당한 스텝의 크기가 달라지며,\n",
    "보통 여러 실혐을 통해 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 이용하여 주어진 데이터들의 분포에 대한 선형 모델을 구하는 방법을\n",
    "**선형회귀**(linear regression)라 부른다. \n",
    "\n",
    "먼저 $y = f(x) = 20*x + 5$ 일차함수의 그래프에 해당하는 데이터를 구한다. \n",
    "여기서 $x$는 -0.5에서 0.5 사이에 있는 100개의 값으로 주어지며,\n",
    "$y$값에 약간의 잡음(가우시안 잡음)이 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x는 -0.5에서 0.5 사이\n",
    "xs = [x/100 for x in range(-50, 50)]\n",
    "\n",
    "# 약간의 잡음 추가 (가우시안 잡음)\n",
    "error = [random.randrange(-100,100)/100 for _ in range(-50, 50)]\n",
    "\n",
    "# y = 20*x + 5 + 가우시안 잡음\n",
    "ys = [20*x + 5 + e for x, e in zip(xs, error)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 프로그램은 잡음이 포함되어 직선으로 그려지지 않는 \n",
    "데이터의 분포를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 그래프를 선형적으로 묘사하는 함수를 경사하강법을 이용하여 구현한다.\n",
    "구현 대상은 아래 모양의 함수이다. \n",
    "\n",
    "$$\n",
    "\\hat y = f(x) = \\theta_0 x + \\theta_1\n",
    "$$\n",
    "\n",
    "* $\\theta_0$: 직선의 기울기\n",
    "* $\\theta_1$: 절편\n",
    "* $\\hat y$: $y$에 대한 예측치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프를 가장 잘 묘사하는 직선을 구해야 한다.\n",
    "즉, 적절한 $\\theta_0$와 $\\theta_1$을 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 적용하려면 최소화 대상함수를 정해야 한다.\n",
    "여기서는 예측치 $\\hat y$와 실제 $y$ 사이의 오차의 **평균제곱오차**(mean squared error, MSE)를\n",
    "계산하는 함수를 사용한다.\n",
    "\n",
    "$$\n",
    "\\textrm{MSE}(\\theta_0, \\theta_1) = \\sum_{y \\in ys} (\\hat y - y)^2\n",
    "= \\sum_{x \\in \\textrm{xs}} (\\theta_0 x + \\theta_1 - y)^2\n",
    "$$\n",
    "\n",
    "즉, MSE를 최소로 하는 $\\theta_0, \\theta_1$을 구해야 한다.\n",
    "MSE의 그레이디언트는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y) x\\, ,\n",
    "\\qquad\n",
    "\\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y)\n",
    "$$\n",
    "\n",
    "아래 코드는 특정 x에 대한 실제 값 y와 예측치 $\\hat y$ 사이의 제곱오차와\n",
    "제곱오차의 그레이디언트를 계산한다.\n",
    "\n",
    "* `slope`: $\\theta_0$ (기울기)\n",
    "* `intercept`: $\\theta_1$ (절편)\n",
    "* `predicted`: $\\hat y$\n",
    "* `error`: $(\\hat y - y)$\n",
    "* `grad`: $(2(\\hat y - y) x, 2(\\hat y - y))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    # 기울기와 절편\n",
    "    slope, intercept = theta\n",
    "    # 예측치\n",
    "    predicted = slope * x + intercept\n",
    "    # 오차\n",
    "    error = (predicted - y)          \n",
    "    # 제곱 오차\n",
    "    squared_error = error ** 2       \n",
    "    # 특정 x에 대한 제곱오차의 그레이디언트 항목\n",
    "    grad = [2 * error * x, 2 * error]\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 $\\theta = (\\theta_0, \\theta_1)$로 시작한 후에\n",
    "경사하강법으로 평균제곱오차의 최솟값 지점을 구할 수 있다.\n",
    "\n",
    "* `vector_mean`: 벡터 항목들의 평균값 계산\n",
    "* `epoch`(에포크): 5000회 반복\n",
    "* `learning_rate`: 스텝 크기. 보통 **학습률**이라 부름.\n",
    "\n",
    "아래 코드에서 사용한 학습률은 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.3960305973284491, -0.3710428077861639]\n",
      "500 [1.2081698200159328, 2.9104805753419383]\n",
      "1000 [2.6939742317026463, 4.12133302369376]\n",
      "1500 [4.0646100896231445, 4.570820537251568]\n",
      "2000 [5.326992692406548, 4.740145907728204]\n",
      "2500 [6.488934564100917, 4.8061804956497625]\n",
      "3000 [7.5581565205864845, 4.8339511929997645]\n",
      "3500 [8.541957930313517, 4.847379799072084]\n",
      "4000 [9.4471265357612, 4.855279995926929]\n",
      "4500 [10.279933835150167, 4.860911423954035]\n",
      "\n",
      "---\n",
      "\n",
      "최종 기울기: 11.045\n",
      "최종 절편: 4.865\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in zip(xs, ys)])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 최종 기울기가 11.398로 애초에 사용한 20과 차이가 크다.\n",
    "이유는 학습률이 너무 낮아서 5000번의 에포크가 충북한 학습을 위해 부족했기 때문이다.\n",
    "이에 대한 해결책은 보통 두 가지이다. \n",
    "\n",
    "* 첫째: 학습률 키우기\n",
    "* 둘째: 에포크 키우기\n",
    "\n",
    "아래 코드는 먼저 에포크를 네 배 늘린 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.17992348494955882, -0.18518165957460925]\n",
      "1000 [2.877622929099383, 4.147277408604718]\n",
      "2000 [5.4825458840467345, 4.744363056415772]\n",
      "3000 [7.6898434907668625, 4.835124442427624]\n",
      "4000 [9.558599731338786, 4.855949470799869]\n",
      "5000 [11.140520579677462, 4.866013948268633]\n",
      "6000 [12.47960304807292, 4.873511880950049]\n",
      "7000 [13.613120940737142, 4.879720826051201]\n",
      "8000 [14.572630251336857, 4.884957990032381]\n",
      "9000 [15.384843310456798, 4.88938866957423]\n",
      "10000 [16.07237185942188, 4.893138846430851]\n",
      "11000 [16.654356474496804, 4.896313279954179]\n",
      "12000 [17.146999423755158, 4.899000393401074]\n",
      "13000 [17.56401573166685, 4.901275001649822]\n",
      "14000 [17.91701500086275, 4.903200429953527]\n",
      "15000 [18.215824619787917, 4.904830281747879]\n",
      "16000 [18.46876335409622, 4.906209931609349]\n",
      "17000 [18.682872938465067, 4.907377788557602]\n",
      "18000 [18.864114116270308, 4.908366365362448]\n",
      "19000 [19.017532584803895, 4.9092031836907255]\n",
      "\n",
      "---\n",
      "\n",
      "최종 기울기: 19.147\n",
      "최종 절편: 4.910\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(20000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in zip(xs, ys)])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 1000번에 한 번 학습과정 확인\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 아래 코드는 학습률을 0.01로 키웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.37546177183351565, -0.6342667828035545]\n",
      "500 [11.059984087125178, 4.865575862194283]\n",
      "1000 [16.039762467337255, 4.8929609800060785]\n",
      "1500 [18.20269145611137, 4.904758647138523]\n",
      "2000 [19.142143036244793, 4.909882869198325]\n",
      "2500 [19.550186608629915, 4.912108535660589]\n",
      "3000 [19.727417197715923, 4.913075236780057]\n",
      "3500 [19.804395941736253, 4.9134951159803855]\n",
      "4000 [19.83783106747562, 4.913677487283489]\n",
      "4500 [19.85235335732888, 4.913756698857131]\n",
      "\n",
      "---\n",
      "\n",
      "최종 기울기: 19.859\n",
      "최종 절편: 4.914\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in zip(xs, ys)])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교했을 때 학습률을 키우는 것이 보다 효과적이다. \n",
    "최종적으로 구해진 기울기와 절편을 이용하여 처음에 주어진 데이터의 분포를\n",
    "선형적으로 학습한 1차함수의 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측치\n",
    "zs = [slope *x + intercept for x in xs]\n",
    "\n",
    "# 실제 데이터 분포\n",
    "plt.plot(xs, ys, 'r.', label='Actuals')\n",
    "# 예측치 그래프\n",
    "plt.plot(xs, zs, 'b-', label='Estimates')\n",
    "\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 4: 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 경사하강법은 주어진 데이터셋 전체를 대상으로 평균제곱오차와 그레이디언트를 계산하였다.\n",
    "이런 방식을 **배치 경사하강법**이라 부른다.\n",
    "\n",
    "그런데 사용된 데이터셋의 크기가 100이었기 때문에 계산이 별로 오래 걸리지 않았지만,\n",
    "데이터셋이 커지면 그러한 계산이 매우 오래 걸릴 수 있다.\n",
    "실전에서 사용되는 데이터셋의 크기는 몇 만에서 수십억까지 다양하며, \n",
    "그런 경우에 적절한 학습률을 찾는 과정이 매우 오래 걸릴 수 있다.\n",
    "\n",
    "데이터셋이 매우 큰 경우에는 따라서 아래 두 가지 방식을 추천한다.\n",
    "\n",
    "* 확률적 경사하강법\n",
    "* 미니배치 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]\n",
    "\n",
    "\n",
    "# Minibatch gradient descent example\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
    "\n",
    "\n",
    "# Stochastic gradient descent example\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
